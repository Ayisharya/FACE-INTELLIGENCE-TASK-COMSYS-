{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3i20S4Al2N7Q",
        "outputId": "9542ba95-394f-43db-996e-abff7f2a8f8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.6.0+cu124)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchvision scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bewhqfHo2wYn",
        "outputId": "99770899-edd0-4f47-815a-2a4695280244"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "zip_path = \"/content/drive/MyDrive/Task_B.zip\"  # Adjust if in subfolder\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content\")\n"
      ],
      "metadata": {
        "id": "7ExHkl3hCUl9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class SiameseNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SiameseNetwork, self).__init__()\n",
        "        base_model = models.resnet18(pretrained=True)\n",
        "        base_model.fc = nn.Identity()  # remove final FC\n",
        "        self.encoder = base_model\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        f1 = self.encoder(x1)\n",
        "        f2 = self.encoder(x2)\n",
        "        dist = torch.abs(f1 - f2)\n",
        "        return self.classifier(dist)\n"
      ],
      "metadata": {
        "id": "uLNmQpV14meG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import random\n",
        "\n",
        "class FaceConSiameseDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.person_folders = [os.path.join(root_dir, f) for f in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, f))]\n",
        "        self.pairs = self.create_pairs()\n",
        "\n",
        "    def create_pairs(self):\n",
        "        pairs = []\n",
        "        for person_dir in self.person_folders:\n",
        "            frontal_img = None\n",
        "            for file in os.listdir(person_dir):\n",
        "                if file.lower().endswith(('.jpg', '.png')) and \"frontal\" in file.lower():\n",
        "                    frontal_img = os.path.join(person_dir, file)\n",
        "                    break\n",
        "\n",
        "            distortion_dir = os.path.join(person_dir, \"distortion\")\n",
        "            if not frontal_img or not os.path.exists(distortion_dir):\n",
        "                continue\n",
        "\n",
        "            distorted_images = [\n",
        "                os.path.join(distortion_dir, f)\n",
        "                for f in os.listdir(distortion_dir)\n",
        "                if f.lower().endswith(('.jpg', '.png'))\n",
        "            ]\n",
        "\n",
        "            for dimg in distorted_images:\n",
        "                pairs.append((frontal_img, dimg, 1))\n",
        "\n",
        "            neg_person = random.choice([p for p in self.person_folders if p != person_dir])\n",
        "            neg_dist_dir = os.path.join(neg_person, \"distortion\")\n",
        "            if os.path.exists(neg_dist_dir):\n",
        "                neg_imgs = [os.path.join(neg_dist_dir, f) for f in os.listdir(neg_dist_dir) if f.lower().endswith(('.jpg', '.png'))]\n",
        "                if neg_imgs:\n",
        "                    neg_img = random.choice(neg_imgs)\n",
        "                    pairs.append((frontal_img, neg_img, 0))\n",
        "\n",
        "        print(f\"âœ… Created {len(pairs)} pairs\")\n",
        "        return pairs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path1, path2, label = self.pairs[idx]\n",
        "        img1 = Image.open(path1).convert(\"RGB\")\n",
        "        img2 = Image.open(path2).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            img1 = self.transform(img1)\n",
        "            img2 = self.transform(img2)\n",
        "\n",
        "        return img1, img2, torch.tensor(label, dtype=torch.float32)\n"
      ],
      "metadata": {
        "id": "sXvB1GrrIjk3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class SiameseNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SiameseNetwork, self).__init__()\n",
        "        resnet = models.resnet18(weights='IMAGENET1K_V1')\n",
        "        resnet.fc = nn.Identity()  # remove final FC\n",
        "        self.backbone = resnet\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        feat1 = self.backbone(x1)\n",
        "        feat2 = self.backbone(x2)\n",
        "        diff = torch.abs(feat1 - feat2)\n",
        "        out = self.fc(diff)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "89Th8tTyIkXe"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_dataset = FaceConSiameseDataset(\"/content/Task_B/train\", transform=transform)\n",
        "val_dataset = FaceConSiameseDataset(\"/content/Task_B/val\", transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SiameseNetwork().to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for x1, x2, labels in loader:\n",
        "            x1, x2, labels = x1.to(device), x2.to(device), labels.to(device)\n",
        "            outputs = model(x1, x2).squeeze()\n",
        "            preds = torch.sigmoid(outputs) > 0.5\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(preds.cpu().numpy())\n",
        "    return {\n",
        "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
        "        \"Precision\": precision_score(y_true, y_pred),\n",
        "        \"Recall\": recall_score(y_true, y_pred),\n",
        "        \"F1\": f1_score(y_true, y_pred)\n",
        "    }\n",
        "\n",
        "# ðŸ” Training Loop\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for x1, x2, labels in train_loader:\n",
        "        x1, x2, labels = x1.to(device), x2.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(x1, x2).squeeze()\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}\")\n",
        "    metrics = evaluate(model, val_loader)\n",
        "    print(\"Validation:\", metrics)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLdJD6mpIpPN",
        "outputId": "af200c72-bf1c-41d9-b192-2beabb62753a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Created 713 pairs\n",
            "âœ… Created 224 pairs\n",
            "Epoch 1, Loss: 0.4636\n",
            "Validation: {'Accuracy': 0.875, 'Precision': 0.875, 'Recall': 1.0, 'F1': 0.9333333333333333}\n",
            "Epoch 2, Loss: 0.1936\n",
            "Validation: {'Accuracy': 0.9955357142857143, 'Precision': 0.9949238578680203, 'Recall': 1.0, 'F1': 0.9974554707379135}\n",
            "Epoch 3, Loss: 0.0834\n",
            "Validation: {'Accuracy': 0.9910714285714286, 'Precision': 0.98989898989899, 'Recall': 1.0, 'F1': 0.9949238578680203}\n",
            "Epoch 4, Loss: 0.0425\n",
            "Validation: {'Accuracy': 0.9910714285714286, 'Precision': 0.98989898989899, 'Recall': 1.0, 'F1': 0.9949238578680203}\n",
            "Epoch 5, Loss: 0.0251\n",
            "Validation: {'Accuracy': 0.9598214285714286, 'Precision': 0.9947089947089947, 'Recall': 0.9591836734693877, 'F1': 0.9766233766233766}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "model.eval()  # no weight update while evaluating training metrics\n",
        "train_preds = []\n",
        "train_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for img1, img2, label in train_loader:\n",
        "        img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
        "\n",
        "        output = model(img1, img2).squeeze()\n",
        "        preds = (torch.sigmoid(output) > 0.5).float()\n",
        "\n",
        "        train_preds.extend(preds.cpu().numpy())\n",
        "        train_labels.extend(label.cpu().numpy())\n",
        "\n",
        "# Compute training metrics\n",
        "train_accuracy  = accuracy_score(train_labels, train_preds)\n",
        "train_precision = precision_score(train_labels, train_preds)\n",
        "train_recall    = recall_score(train_labels, train_preds)\n",
        "train_f1        = f1_score(train_labels, train_preds)\n",
        "\n",
        "print(\"âœ… Training Metrics\")\n",
        "print(f\"Accuracy:  {train_accuracy:.4f}\")\n",
        "print(f\"Precision: {train_precision:.4f}\")\n",
        "print(f\"Recall:    {train_recall:.4f}\")\n",
        "print(f\"F1 Score:  {train_f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cU8eV638e2vN",
        "outputId": "d54c3e7a-fb17-4ffd-a540-3476193f896e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Training Metrics\n",
            "Accuracy:  0.9719\n",
            "Precision: 0.9855\n",
            "Recall:    0.9824\n",
            "F1 Score:  0.9839\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "train_results = {\n",
        "    \"train_accuracy\": round(train_accuracy, 4),\n",
        "    \"train_precision\": round(train_precision, 4),\n",
        "    \"train_recall\": round(train_recall, 4),\n",
        "    \"train_f1_score\": round(train_f1, 4)\n",
        "}\n",
        "\n",
        "with open(\"results_task_b_train.json\", \"w\") as f:\n",
        "    json.dump(train_results, f, indent=4)\n",
        "\n",
        "print(\"âœ… Saved training metrics to results_task_b_train.json\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inwYJqNofBwe",
        "outputId": "d6f4e64d-35d1-47da-c6d9-7ec638776376"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Saved training metrics to results_task_b_train.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KGFYvUhLosKz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}